{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot-product attention (especially a specific use of it known as self-attention) is a major component of the transformer architecture. The very high-level idea of attention is usually easy to grasp, but I find that the deeper intuition behind attention is a little hard to tease out of the conventional mathematical notation used to describe/implement. I suspect this is due to the mathematical notation likely following from the implementation where matrix multiplication is used for obvious reasons of efficiency and compactness. However, I find that \"unraveling\" dot-product attention matrix operations to a point of vector and scalar operations under a few for-loops helped me facilitate a better understanding of what is going on. Specifically, I found this crucial to more clearly think about how the attention mechanism moves information around tokens. After that, it is easy to think about attention as a specific method of \"token mixing\" for which there could be many alternatives [[1]](https://arxiv.org/abs/2111.11418) (i.e. pooling , MLP mixer [[2]](https://arxiv.org/abs/2105.01601), etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine we have 10 tokens, each represented with an 256-D embedding\n",
    "x = torch.randn(10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the sequence into query, key, and value matrices\n",
    "q_proj = nn.Linear(256, 64)\n",
    "k_proj = nn.Linear(256, 64)\n",
    "v_proj = nn.Linear(256, 64)\n",
    "# we could pack this into one linear layer and split the output i.e. nn.Linear(256, 3 * 64), but doing it separately just for clarity\n",
    "\n",
    "queries = q_proj(x)  # shape = (10, 64)\n",
    "keys = k_proj(x)  # shape = (10, 64)\n",
    "values = v_proj(x)  # shape = (10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vectorized dot product attention\n",
    "att = queries @ keys.transpose(-2, -1)  # shape = (10, 10)\n",
    "att = F.softmax(att, dim=-1)\n",
    "out_vectorized = att @ values  # shape = (10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's unravel this a bit in order to gain a better intuition of what is going on\n",
    "\n",
    "# init an empty matrix to collect dot products\n",
    "att = torch.zeros(10, 10)\n",
    "\n",
    "\n",
    "for q_idx, q in enumerate(queries):\n",
    "    # for the current token q we want to compute the dot product against every other token\n",
    "    for k_idx, k in enumerate(keys):\n",
    "        qk_similarity = torch.dot(q, k)\n",
    "        att[q_idx, k_idx] = qk_similarity\n",
    "\n",
    "# softmax so the sum of each row is 1, i.e. att[0].sum() == 1\n",
    "att = F.softmax(att, dim=-1)\n",
    "# each row corresponds to our tokens while the column values tell us how much a token attends to every other token\n",
    "\n",
    "# init empty out matrix to collect weighted sums of token embedddings\n",
    "out_looped = torch.zeros(10, 64)\n",
    "\n",
    "for row, a in enumerate(att):\n",
    "    # for each token we look at how much it attended to all the other tokens (including itself) and use those attention \n",
    "    # values to do a weighted sum of all the token embeddings, which then becomes the new embedding for the current token\n",
    "\n",
    "    # init intermediate version of the values that we will weight with the current token's attention values\n",
    "    weighted_values = torch.zeros_like(values)  # shape = (10, 64)\n",
    "    for v_idx, v in enumerate(values):\n",
    "        # apply the attention weight\n",
    "        weighted_values[v_idx] = a[v_idx] * v\n",
    "\n",
    "    # complete the weighted sum for the current token\n",
    "    weighted_values_sum = weighted_values.sum(0)  # shape = (64,)\n",
    "\n",
    "    # now we have a new embedding for the current token!\n",
    "    out_looped[row] = weighted_values_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check if our looped output matches our vectorized output\n",
    "torch.allclose(out_vectorized, out_looped, atol=0.000001)  # up to some precision difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most language modeling settings, we want tokens to only attend to itself and the previous tokens in the sequence, but not any token after itself in the sequence. \n",
    "\n",
    "i.e. for the sequence \"The quick brown fox ...\", we want \"brown\" to attend to \"brown\", \"quick\" and \"The\", but not \"fox\"\n",
    "\n",
    "We can achieve this in our loop solution by skipping the dot product when `k_idx > q_idx` and setting the attention value to -inf (because this will be converted to 0.0 by the softmax later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init an empty matrix to collect dot products\n",
    "att = torch.zeros(10, 10)\n",
    "\n",
    "\n",
    "for q_idx, q in enumerate(queries):\n",
    "    # for the current token q we want to compute the dot product against every other token (in the form of the keys)\n",
    "    for k_idx, k in enumerate(keys):\n",
    "        # restricts attention to current token + previous token, but disallows attention to future tokens\n",
    "        if k_idx > q_idx:\n",
    "            att[q_idx, k_idx] = float('-inf')\n",
    "            continue\n",
    "        qk_similarity = torch.dot(q, k)\n",
    "        att[q_idx, k_idx] = qk_similarity\n",
    "\n",
    "# softmax so the sum of each row is 1, i.e. att[0].sum() == 1\n",
    "att = F.softmax(att, dim=-1)\n",
    "# each row corresponds to our tokens while the column values tell us how much a token attends to every other token\n",
    "\n",
    "# init empty out matrix to collect weighted sums of token embedddings\n",
    "out_looped = torch.zeros(10, 64)\n",
    "\n",
    "for row, a in enumerate(att):\n",
    "    # for each token we look at how much it attended to all the other tokens (including itself...hence self-attention) and use those attention values to do a weighted sum of all the token embeddings, which then becomes the new embedding for the current token\n",
    "\n",
    "    # init intermediate version of the values that we will weight with the current token's attention values\n",
    "    weighted_values = torch.zeros_like(values)  # shape = (10, 64)\n",
    "    for v_idx, v in enumerate(values):\n",
    "        # apply the attention weight\n",
    "        weighted_values[v_idx] = a[v_idx] * v\n",
    "\n",
    "    # complete the weighted sum for the current token\n",
    "    weighted_values_sum = weighted_values.sum(0)  # shape = (64,)\n",
    "\n",
    "    # now we have a new embedding for the current token!\n",
    "    out_looped[row] = weighted_values_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is how we can achieve this in our vectorized solution\n",
    "causal_mask = torch.tril(torch.ones(10, 10))\n",
    "att = queries @ keys.transpose(-2, -1)\n",
    "att = att.masked_fill(causal_mask == 0, float('-inf'))\n",
    "att = F.softmax(att, dim=-1)\n",
    "out_vectorized = att @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check if our looped output matches our vectorized output\n",
    "torch.allclose(out_vectorized, out_looped, atol=0.000001)  # up to some precision difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1e20b3ef1b838da9104aeb6cd4d0c7647ddc69a5dfe686a019490760929c6c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
