{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from torch import Tensor\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_type)  # NOTE: this will download the model weights which are ~200-300 MB for GPT2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a generic process for generating text from our language model that should work regardless of the decoding strategies we'll cover here. We start with an input prompt provided by a user, that prompt is fed to through the model to obtain probabilities for every token in the model's vocabulary in order to predict what should be the next token in the sequence. A token is selected from according to the predicted probabilities based on some decoding strategy. The predicted token is then appended to the prompt and the update sequence is fed to the model again to predict another token. This process is repeated until some pre-defined maximum number of steps is reached or until an end of sentence (EOS) token is predicted.\n",
    "\n",
    "(note that huggingface's transformer library already has functionality to do all this with better code and interfaces but it's a fun learning experience to implement these methods from scratch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, model: nn.Module, tokenizer: PreTrainedTokenizer, decoding_strategy: 'DecodingStrategy') -> None:\n",
    "        # our huggingface transformer model that out puts next token probabilities for each timestep\n",
    "        self.model = model\n",
    "        # our tokenizer for the model we're using\n",
    "        self.tokenizer = tokenizer\n",
    "        # our decoding strategy which we will define later\n",
    "        self.decoding_strategy = decoding_strategy\n",
    "\n",
    "        # the index in the logits that corrresponds to our end of sentence token\n",
    "        self.eos_idx = tokenizer.eos_token_id\n",
    "        # the maximum timesteps allowed by our model for one input sequence (i.e. 1024)\n",
    "        self.max_input_length = model.config.max_position_embeddings\n",
    "\n",
    "    def generate(self, prompt: str, max_steps: int = 100, temperature: int = 1.0) -> str:\n",
    "        # convert an input string into token IDs\n",
    "        x = self.tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "\n",
    "        # keep a full record of prompt and all predicted tokens to construct intermediate inputs and final output\n",
    "        running_out = [x]\n",
    "\n",
    "        step_count = 0\n",
    "        max_length_warning_triggered = False\n",
    "        while step_count < max_steps:\n",
    "            # truncate input sequnce if it violates maximum length by taking N most recent tokens\n",
    "            if x.size(1) > self.max_input_length:\n",
    "                # only want to issue this warning once\n",
    "                if not max_length_warning_triggered:\n",
    "                    warnings.warn(f'Max input length for model exceeded, using most recent {self.max_input_length} tokens for remaining steps.')\n",
    "                x = x[:, x.size(1)-self.max_input_length:]\n",
    "                max_length_warning_triggered = True\n",
    "\n",
    "            # run forward pass through our model\n",
    "            out = model(x)\n",
    "            # batch size of 1 and take logits from final token in the sequence to get predictions about next token\n",
    "            logits = out.logits[0, -1]\n",
    "            # convert to probability distribution and temperature scaling\n",
    "            probs = F.softmax(logits / temperature, dim=0)\n",
    "\n",
    "            # run decoding strategy to select predicted next token\n",
    "            token_idx = self.decoding_strategy.sample(probs)\n",
    "\n",
    "            # end sampling if we select the end of sentence token\n",
    "            if token_idx.item() == self.eos_idx:\n",
    "                break\n",
    "\n",
    "            # append predicted token to our sequence in prep for next prediction step\n",
    "            running_out.append(token_idx.view(1, 1))\n",
    "            x = torch.cat(running_out, dim=1)\n",
    "            step_count += 1\n",
    "\n",
    "        # convert input prompt and predicted token IDs into final output string\n",
    "        return self.tokenizer.decode(torch.cat(running_out, dim=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a simple interface that our decoding strategies will adhere to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def sample(self, probs: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyDecoding(DecodingStrategy):\n",
    "    def sample(self, probs: Tensor) -> Tensor:\n",
    "        # greedy sampling - we just use the token with the highest probability\n",
    "        token_idx = probs.argmax()\n",
    "\n",
    "        return token_idx\n",
    "\n",
    "\n",
    "decoding_strategy = GreedyDecoding()\n",
    "gen = TextGenerator(model, tokenizer, decoding_strategy)\n",
    "\n",
    "print(gen.generate('A layer of ice; it feels rough against my face, but not cold.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDecoding(DecodingStrategy):\n",
    "    def sample(self, probs: Tensor) -> Tensor:\n",
    "        # random sampling - select a random token according to the predicted token probability distribution\n",
    "        token_idx = probs.multinomial(num_samples=1)\n",
    "\n",
    "        return token_idx\n",
    "\n",
    "\n",
    "decoding_strategy = RandomDecoding()\n",
    "gen = TextGenerator(model, tokenizer, decoding_strategy)\n",
    "\n",
    "print(gen.generate('A layer of ice; it feels rough against my face, but not cold.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKDecoding(DecodingStrategy):\n",
    "    def __init__(self, k: int) -> None:\n",
    "        self.k = k\n",
    "\n",
    "    def sample(self, probs: Tensor) -> Tensor:\n",
    "        # top K sampling - take the top K highest probability tokens, re-normalize their probabilities, do weighted random sampling from that\n",
    "        _, sorted_indices = torch.sort(probs, descending=True)\n",
    "        # set non-top K probabilities to 0.0 to avoid sampling them\n",
    "        probs[sorted_indices[self.k:]] = 0.0\n",
    "        # renormalize probabilities to sum to 1.0\n",
    "        probs = probs / probs.sum()\n",
    "        # sample from new probability distribution which is now restricted to the top K tokens\n",
    "        token_idx = probs.multinomial(num_samples=1)\n",
    "\n",
    "        return token_idx\n",
    "\n",
    "\n",
    "decoding_strategy = TopKDecoding(k=100)\n",
    "gen = TextGenerator(model, tokenizer, decoding_strategy)\n",
    "\n",
    "print(gen.generate('A layer of ice; it feels rough against my face, but not cold.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top P sampling (a.k.a. Nucleus sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopPDecoding(DecodingStrategy):\n",
    "    \"\"\"code ref: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\"\"\"\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "\n",
    "    def sample(self, probs: Tensor) -> Tensor:\n",
    "        # top P sampling (or nucleus sampling) - take the top highest probability tokens that add up to P, re-normalize their probabilities, do random sampling from that new distribution\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        # find top probabilities until the sum of them exceeds P\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        idx_to_suppress = cumulative_probs > self.p\n",
    "        # if the single highest probability is already > P then we just select that token\n",
    "        idx_to_suppress[0] = False\n",
    "\n",
    "        # set non-top P probabilities to 0.0 to avoid sampling them\n",
    "        probs[sorted_indices[idx_to_suppress == True]] = 0.0\n",
    "        # renormalize probabilities to sum to 1.0\n",
    "        probs = probs / probs.sum()\n",
    "        # sample from new probability distribution which is now restricted to the top P tokens\n",
    "        token_idx = probs.multinomial(num_samples=1)\n",
    "\n",
    "        return token_idx\n",
    "\n",
    "\n",
    "\n",
    "decoding_strategy = TopPDecoding(p=0.75)\n",
    "gen = TextGenerator(model, tokenizer, decoding_strategy)\n",
    "\n",
    "print(gen.generate('A layer of ice; it feels rough against my face, but not cold.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K followed by Top P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKTopPDecoding(DecodingStrategy):\n",
    "    def __init__(self, k: int, p: float) -> None:\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "\n",
    "    def sample(self, probs: Tensor) -> Tensor:\n",
    "        # top K sampling\n",
    "        _, sorted_indices = torch.sort(probs, descending=True)\n",
    "        probs[sorted_indices[self.k:]] = 0.0\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "        # top P sampling on top K\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        idx_to_suppress = cumulative_probs > self.p\n",
    "        idx_to_suppress[0] = False\n",
    "\n",
    "        probs[sorted_indices[idx_to_suppress == True]] = 0.0\n",
    "        probs = probs / probs.sum()\n",
    "        token_idx = probs.multinomial(num_samples=1)\n",
    "\n",
    "\n",
    "        return token_idx\n",
    "\n",
    "\n",
    "decoding_strategy = TopKTopPDecoding(k=1000, p=0.75)\n",
    "gen = TextGenerator(model, tokenizer, decoding_strategy)\n",
    "\n",
    "print(gen.generate('A layer of ice; it feels rough against my face, but not cold.'))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Beam Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1e20b3ef1b838da9104aeb6cd4d0c7647ddc69a5dfe686a019490760929c6c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
